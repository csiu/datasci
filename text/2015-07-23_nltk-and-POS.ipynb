{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose:** To experiment with Python's [Natural Language Toolkit](http://www.nltk.org).\n",
    "\n",
    "> NLTK is a leading platform for building Python programs to work with human language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bloboftext = \"\"\"\n",
    "This little piggy went to market,\n",
    "This little piggy stayed home,\n",
    "This little piggy had roast beef,\n",
    "This little piggy had none,\n",
    "And this little piggy went wee wee wee all the way home.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "- **Tokenization** to break text into units e.g. words, phrases, or symbols\n",
    "- **Stop word removal** to get rid of common words \n",
    "  - e.g. this, a, is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "## Tokenization \n",
    "bagofwords = nltk.word_tokenize(bloboftext.lower())\n",
    "print len(bagofwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "## Stop word removal\n",
    "stop = stopwords.words('english')\n",
    "bagofwords = [i for i in bagofwords if i not in stop]\n",
    "print len(bagofwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About stemmers and lemmatisation \n",
    "- **Stemming** to reduce a word to its roots \n",
    "  - e.g. having => hav\n",
    "\n",
    "- **Lemmatisation** to determine a word's lemma/canonical form \n",
    "  - e.g. having => have\n",
    "\n",
    "\n",
    "> [English Stemmers and Lemmatizers](http://text-processing.com/demo/stem/:)\n",
    "> \n",
    "> For stemming English words with NLTK, you can choose between the **PorterStemmer** or the **LancasterStemmer**. The [Porter Stemming Algorithm](http://tartarus.org/~martin/PorterStemmer/) is the oldest stemming algorithm supported in NLTK, originally published in 1979. The [Lancaster Stemming Algorithm]() is much newer, published in 1990, and can be more aggressive than the Porter stemming algorithm.\n",
    "> \n",
    "> The **WordNet Lemmatizer** uses the [WordNet Database](http://wordnet.princeton.edu) to lookup lemmas. Lemmas differ from stems in that a lemma is a canonical form of the word, while a stem may not be a real word.\n",
    "\n",
    "- Resources:\n",
    "  - [PorterStemmer or the SnowballStemmer](http://www.nltk.org/howto/stem.html) (Snowball == Porter2)\n",
    "  - [Stemming and Lemmatization](http://textminingonline.com/tag/lancaster-stemmer)\n",
    "  - [What are the major differences and benefits of Porter and Lancaster Stemming algorithms?](http://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE:\t\"little\", \"piggy\", \"stayed\"\n",
      " AFTER:\t\"piggi\", \"littl\", \"stay\"\n"
     ]
    }
   ],
   "source": [
    "snowball_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "## What words was stemmed?\n",
    "_original = set(bagofwords) \n",
    "_stemmed  = set([snowball_stemmer.stem(i) for i in bagofwords])\n",
    "\n",
    "print 'BEFORE:\\t%s' % ', '.join(map(lambda x:'\"%s\"'%x, _original-_stemmed))\n",
    "print ' AFTER:\\t%s' % ', '.join(map(lambda x:'\"%s\"'%x, _stemmed-_original))\n",
    "\n",
    "del _original, _stemmed\n",
    "\n",
    "## Proceed with stemming\n",
    "bagofwords = [snowball_stemmer.stem(i) for i in bagofwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count & POS tag of each stemmed/non-stop word\n",
    "- meaning of POS tags: [Penn Part of Speech Tags](http://cs.nyu.edu/grishman/jet/guide/PennPOS.html)\n",
    "```\n",
    "NN\t Noun, singular or mass\n",
    "VBD\tVerb, past tense\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\tNN\tpiggi\n",
      "5\tNN\tlittl\n",
      "4\t,\t,\n",
      "3\tNN\twee\n",
      "2\tNN\thome\n",
      "2\tVBD\twent\n",
      "1\tNN\tnone\n",
      "1\tNN\tbeef\n",
      "1\tNN\tstay\n",
      "1\tNN\tway\n",
      "1\tNN\troast\n",
      "1\t.\t.\n",
      "1\tNN\tmarket\n"
     ]
    }
   ],
   "source": [
    "for token, count in Counter(bagofwords).most_common():\n",
    "    print '%d\\t%s\\t%s' % (count, nltk.pos_tag([token])[0][1], token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proportion of POS tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>  4</td>\n",
       "      <td> 14.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>  1</td>\n",
       "      <td>  3.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td> 21</td>\n",
       "      <td> 75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBD</th>\n",
       "      <td>  2</td>\n",
       "      <td>  7.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     count    percent\n",
       ",        4  14.285714\n",
       ".        1   3.571429\n",
       "NN      21  75.000000\n",
       "VBD      2   7.142857"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record = {}\n",
    "for token, count in Counter(bagofwords).most_common():\n",
    "    postag = nltk.pos_tag([token])[0][1]\n",
    "\n",
    "    if record.has_key(postag):\n",
    "        record[postag] += count\n",
    "    else:\n",
    "        record[postag] = count\n",
    "\n",
    "recordpd = pd.DataFrame.from_dict([record]).T\n",
    "recordpd.columns = ['count']\n",
    "N = sum(recordpd['count'])\n",
    "recordpd['percent'] = recordpd['count']/N*100\n",
    "recordpd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
