{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document classification\n",
    "- data is stored in textual format\n",
    "- application in information management + document retrieval, routing, or filtering system\n",
    "  - document indexing based on controlled vocab\n",
    "  - document filtering: relevant vs irrelevant\n",
    "  - automated metadata generation\n",
    "  - word sense disambiguation\n",
    "  - population of hierarchical catalogues of web resources\n",
    "  - speech categorization\n",
    "  - multimedia doc categorization\n",
    "  - author identification\n",
    "  - language identification\n",
    "  - patients into categories\n",
    "  - grouping of conferences papers\n",
    "- Advantages of ML based classification:\n",
    "    - savings in terms of expert labour power\n",
    "    - portability to other domains (there are many domains)\n",
    "    - lots of doc; manual implausible\n",
    "- challenges:\n",
    "    - semantics, homonymns\n",
    "    - high-dimensionality problem\n",
    "    - document representation, construction, classification\n",
    "- single vs multilabel\n",
    "  - single more general\n",
    "- category-pivoted vs document-pivoted text classification\n",
    "  - DP: find all category for the document\n",
    "  - CP: find all document for this category\n",
    "- hard vs rank classifcation\n",
    "    - Binary vs threshold\n",
    "    - threshold obtained analytically vs experimentally\n",
    "\n",
    "# Representation\n",
    "- Transform full text into document vector\n",
    "    - vector space model which is represented by vector of words\n",
    "    - Bag of word/Vector space model\n",
    "        - Disadvantages:\n",
    "            high-dimensionality & loss of info (correlation with adjacent word, sematic relationships)\n",
    "        - To overcome problem:\n",
    "            term weighing\n",
    "- types of document representation\n",
    "    - N-gram\n",
    "    - single terms\n",
    "    - phrases\n",
    "    - RDR (logic-based documentation representation)\n",
    "- Term weighing metrics:\n",
    "    - Boolean weighing\n",
    "    - word frequency weighing\n",
    "    - TF-IDF\n",
    "    - entropy\n",
    "\n",
    "- semantic and ontology base document representation\n",
    "    - Advantages:\n",
    "        - better classification when consider the sematics (via ontology)\n",
    "        - ontology can be used to provide expert, background knowledge\n",
    "    - Disadvantages:\n",
    "        - neeed for large number of training text terms\n",
    "        - translatability between languages\n",
    "        - is challenge to semantically represent text\n",
    "        - polysemy, synonymy\n",
    "\n",
    "- indexing: procedure to map text doc into compact rep of its content\n",
    "  - lexical semantics? choice of meaningful units\n",
    "  - compositional semantics? how to combine units\n",
    "      - usually disregarded in TC ... instead use vector weights e.g. TD-IDF\n",
    "- Darmstadt indexing approach vs bag of words approach\n",
    "    - DIA: use a wider set of features e.g. location of term in document, cetainty\n",
    "\n",
    "\n",
    "## Pre-processing\n",
    "- Tokenization\n",
    "- Removing stop words\n",
    "- Stemming\n",
    "    - morphological root\n",
    "    - sometimes reported to hurt effectiveness\n",
    "\n",
    "## Feature selection\n",
    "- dimensionality reduction: omitting unimportant features\n",
    "    - feature extraction vs feature selection methods\n",
    "    - prevent overfitting\n",
    "- Select subset of features by some criterion; keep words with highest score according to predetermined measure of importance of the word\n",
    "    - Information gain*\n",
    "    - Term frequency\n",
    "    - Chi-square*\n",
    "    - Expected cross entropy\n",
    "    - Odds Ratio\n",
    "    - the weight of evidence of text\n",
    "    - Mutual information\n",
    "    - Gini index\n",
    "    - gentic algorithm (GA) optimization (?)\n",
    "    - Gain ratio\n",
    "    - Conditional mutual information\n",
    "    - Doc freq\n",
    "    - Inverse doc freq\n",
    "    - Term\n",
    "    - Weighted Ratio\n",
    "    - DIA association factor\n",
    "    - NGL coefficient\n",
    "    - relevancy score\n",
    "    - GSS coefficient\n",
    "\n",
    "_* denotes most commonly used_\n",
    "\n",
    "- wrapper vs filter methods\n",
    "    - wrappers: use classification accuracy of learning algorithm as their evaluation function\n",
    "        - not suitable for text classification because of high-dimensionality\n",
    "    - filters: independent of learning algorithm; use evaluation metric\n",
    "- Other approaches\n",
    "    - reduce termset by factor of 10\n",
    "    - remove all terms occurring in at most x training doc\n",
    "- feature selection should consider domain and algorithm characteristics\n",
    "\n",
    "## Feature extraction\n",
    "- clustering\n",
    "- Latent semantic indexing\n",
    "    - lower dimensions obtained looking at patterns of co-occurrences in original dimension\n",
    "    - SVD\n",
    "    - bring out latent semantic of vocabulary\n",
    "    - LSI vs X^2\n",
    "- learning process: finding attributes in examples that distinguish object of separate classes\n",
    "    - avoid overfitting; do cross-validation\n",
    "\n",
    "\n",
    "# Classifiers\n",
    "## Linear Least Square Fit\n",
    "- mapping approach\n",
    "- input/output vector pair\n",
    "- regression method\n",
    "- Disadvantage:\n",
    "    - computational cost\n",
    "\n",
    "## Rocchio's Algorithm\n",
    "- for use in relevance feedback in IR\n",
    "- vector space method\n",
    "- centroid; average vector\n",
    "- inductive learning process\n",
    "- Advantage:\n",
    "    - easy to implement\n",
    "    - low computational cost/efficient\n",
    "    - fast learner\n",
    "    - readily understandable by human than neural network\n",
    "- Disadvantage:\n",
    "    - low classification accuracy when categroy tend to occur in disjoint clusters\n",
    "    - linear classification is too simple\n",
    "\n",
    "## KNN\n",
    "- case-based learning / instant-based learning algorithm\n",
    "- example based classifiers/lazy learners\n",
    "- based on closest feature space\n",
    "- Euclidean distance / Cosine similarity\n",
    "- majority voting\n",
    "- Advantage:\n",
    "    - nonparametric\n",
    "    - simple and easy to implement\n",
    "    - one of fastest ML algorithm\n",
    "    - does not divide space linearly like Rocchio's\n",
    "- Disadvantage:\n",
    "    - classification time is long\n",
    "    - optimal k?\n",
    "    - uses all features\n",
    "    - need to compute distances using all document features\n",
    "    - not robust to noise and irrelevant features\n",
    "    - computationally expensive\n",
    "\n",
    "## Bayesian classifier/Naive Bayes\n",
    "- module classifier\n",
    "- multivariate Bernoulli and multiomial model\n",
    "- spam filtering & email categorization\n",
    "- Advantage:\n",
    "    - easy implementation & computation\n",
    "    - need small amount of training data\n",
    "    - gives good results as long as correct category is more probable than other categories\n",
    "- Disadvantage:\n",
    "    - Poor performance when features are correlated\n",
    "    - relatively low classification performance compare to other discriminative algorithms (e.g. SVM)\n",
    "\n",
    "## Decision Rule\n",
    "- rule-based inference\n",
    "- DNF model\n",
    "- inductive learning rule\n",
    "- heuristics to reduce number of features\n",
    "- Bottom up\n",
    "- Advantage:\n",
    "    - ability to create local dictionary for each separate category\n",
    "    - can give goof effectiveness results\n",
    "    - tend to be more compact classifier than DT\n",
    "- Disadvantage:\n",
    "    - inability to assign a document to a single category\n",
    "    - need help from human experts\n",
    "    - do not work correctly with large features\n",
    "    - knowledge acquisition bottleneck\n",
    "    - rules must be manually defined/updated by knowledge engineer\n",
    "\n",
    "## Decision Tree\n",
    "- entropy criterion\n",
    "- Advantage:\n",
    "    - easy to understand, interpret, and replicate\n",
    "- Disadvantage:\n",
    "    - overfitting\n",
    "- Examples: ID3, C4.5, C5\n",
    "- Top down; divide and conquer\n",
    "- IG or entropy\n",
    "\n",
    "## SVM\n",
    "- need both positive and negative training set\n",
    "- decision surface, hyper plane, support vector\n",
    "- generally binary\n",
    "- discriminative classification method\n",
    "- Advantage:\n",
    "    - at space of large number of dimensions, eliminates least important features\n",
    "    - term selection is often not needed\n",
    "    - no parameter tuning on validation set\n",
    "- Disadvantage:\n",
    "    - high complexity of training and categorization algorithm\n",
    "    - parameter optimization\n",
    "    - kernel selection\n",
    "    - need for both positive and negative training data\n",
    "    - high time and memory consumption during training\n",
    "\n",
    "## Neural Networks\n",
    "- input = terms (can be much greater); output = category\n",
    "- term weights assigned to input units, activation propagated forward\n",
    "- single-layer perceptron, multi-layer perceptron, BPNN, MBPNN\n",
    "- Advantage:\n",
    "    - abilitiy to work with large sets of features\n",
    "    - able to correct classification in presence of noise\n",
    "    - suitable for both discrete and continuous data\n",
    "- Disadvantage:\n",
    "    - large computational cost\n",
    "    - mysterious for typical user\n",
    "\n",
    "## Classifier Committee/Ensemble/Voting\n",
    "- idea k experts may be better than one\n",
    "- combining the experts:\n",
    "    - majority voting\n",
    "    - second weighted majority\n",
    "    - dynamic classifier selection\n",
    "- boosting - train weak learner sequentially; get benefit from prevous iteration\n",
    "- Advantage:\n",
    "    - profit from complementary strength\n",
    "- Disadvantages:\n",
    "    - choice of k classifiers\n",
    "    - combination function:\n",
    "        - majority voting\n",
    "        - weighted linear combination\n",
    "        - dynamic classifier selection (choose classifier that did best in validiation set)\n",
    "        - adaptive classifier combination (summing the judgement of all + weighted by effectiveness on validation example)\n",
    "    - computationally expensive\n",
    "\n",
    "## Other\n",
    "### Latent Semantic Indexing\n",
    "### Fuzzy correlation\n",
    "- can deal with fuzzy information or incomplete data\n",
    "- fuzzy SVM, fuzzy kNN, fuzzy rules\n",
    "\n",
    "### Genetic algorithm\n",
    "- aim to find optimumm characteristic parameters\n",
    "- is an adaptive probability global optimization algorithm\n",
    "\n",
    "\n",
    "## Evaluation\n",
    "- Precision & Recall\n",
    "- Micro-averaging vs Macro-averaging\n",
    "\n",
    "- Combined effectiveness measures\n",
    "    - eleven-point average precision\n",
    "    - breakeven\n",
    "    - FÃŸ-function\n",
    "\n",
    "- Fallout = $\\frac{FNi}{FNi + TNi}$\n",
    "- Error = $\\frac{FNi+FPi}{TPi+FNi+FPi+TNi}$\n",
    "- Accuracy = $\\frac{TPi+TNi}{TPi+FNi+FPi+TNi}$\n",
    "    - not good metric\n",
    "    - trivial rejector will do well\n",
    "- Alternative measures to effectiveness\n",
    "    - efficiency\n",
    "    - utility\n",
    "\n",
    "- train-and-test\n",
    "- k-fold CV\n",
    "- test and validation set must be kept separate\n",
    "\n",
    "## Document collection\n",
    "- Reuters-21578\n",
    "- Reuters Corpus Version 1\n",
    "- 20 Newsgroups data\n",
    "- OHSUMED\n",
    "- AP collection\n",
    "\n",
    "# Memo\n",
    "- see Korde 2012 for table of classifier advantages & disadvantages\n",
    "- see Bilski 2011 for mini blurbs about description, advantages & disadvantages; like Khan et al 2010\n",
    "- see Khan et al 2010 for more embedded description, advantages & disadvantages\n",
    "- 6000+ citations from Sebastiani et al 2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
